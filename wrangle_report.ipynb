{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "35fe316a",
   "metadata": {},
   "source": [
    "# Reporting: wragle_report"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e108682",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "This project used data about the twitter account WeRateDogs in order to create an analysis. In order to perform this analysis data had to be gathered from twitter using the archive tweet ID's, assessed for quality and tidiness issues, then cleaned to correct thes issues that were found to allow for proper analysis and visualization."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62f359d9",
   "metadata": {},
   "source": [
    "## Gathering\n",
    " In order to perform an analysis there were three separate pieces of data that needed to be gathered from different sources and then converted into usable dataframes.\n",
    "\n",
    "1. First I Downloaded the WeRateDogs Twitter archive data (twitter_archive_enhanced.csv) directly from Udacity and then uploaded the file into a Jupyter notebook and read the file into a pandas DataFrame. The twitter archive data included 5,000+ tweets with limited other information but was enhanced by extracted tweet data to create information about dog stage's, dog ratings, and dog name's.\n",
    "\n",
    "2. The next set of data was gathered by using the python Requests library to download the tweet image prediction's (image_predictions.tsv) from the website URL that was provided and hosted by Udacity. This Image Predictions File has data to classify the breed of dog found in each tweet image. \n",
    "\n",
    "3. The final set of data was gathered directly from Twitter through the use of pythons Tweepy library and Tweepy Client method to query additional data via the Twitter API. This was done by creating a loop to pull the tweet ID's from the provided twitter_archive_enhanced.csv and then retrieve the public metrics data from twitter( which included information on retweets, likes, quotes and replies) for the tweets that matched each tweet ID. This data was then saved as JSON data in the file tweet_json.txt and converted into a dataframe. The data from the Tweepy Client method was received as json objects that were hidden as strings and also inside an array object. To fix this I converted each line from a string to json/dictionary array object, selected the first element of the array and appended it to a list of dictionaries, then converted the new list of dictionaries to a dataframe."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56203e80",
   "metadata": {},
   "source": [
    "## Assesing\n",
    "Once all the data was gathered it was then assessed both programmatically and visually through Jupyter notebooks and seperately with excell. After assessing each dataframe notes were made in the Quality Issues section for easier reading later. While assessing the data multiple methods were used on each set of data to ensure multiple angles of the data were looked at for better clarity."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2caae96",
   "metadata": {},
   "source": [
    "### Quality Issues\n",
    "There were several quality issues found in each set of data that was discovered using visual and programmatic assessments: \n",
    "#### Twitter Archive Data:\n",
    "> * The Twitter archive data had some issues with completeness where many columns had null values. \n",
    "> * Issues with accuracy where the ratings columns were missing the correct scores given in the text, but still showed valid data. \n",
    "> * There were also some issues with the validity of this data, specifically with the name column which had words pulled as names that were not names(eg. 'a', 'an', 'the') and with retweet data which was not needed for this analysis. \n",
    "> * The columns doggo, floofer, pupper and puppo had consistency issues causing many values of 'None', which also should have been null values. These columns are all dog stages that could be merged into a single column removing many nulls. There was also consistency issues with datatypes across the tables, this table had timestamps as strings rather than datetime variables.\n",
    "\n",
    "#### Data From Twitter (tweets_df): \n",
    "> * The data from twitter had initial issues with completeness due to errors that occurred while gathering tweets, the tweets were likely unable to be found on twitter due the account or tweet being deleted. The tweets_df table showed 2325 rows vs 2356 in 'twitter_archive' table.\n",
    "> * This data also had some validity issues as it included some retweets based on the text showing 'RT'.\n",
    "> * The datatype of the ID column was a string variable rather than an int like the other tables causing consistency issues across the three tables.\n",
    "#### Predictions Data\n",
    "> * This data had less issues overall for this analysis with some inconsistent values in the dog breed columns p1,p2,p3 where the type of dog breed was predicted as something other than a dog as well as the use of lowercase and uppercase making matching difficult. There was also some missing data as the file only had 2075 rows compared to the other tables which had more."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "426d0881",
   "metadata": {},
   "source": [
    "### Tidiness Issues:\n",
    "\n",
    "There were several issues with the overall tidiness of this data including: \n",
    "\n",
    "> * Unnecessary data from tweets_df(eg. edit_history_tweet_ids)\n",
    "> * The column names for tweets_df, when gathered included 'public_metrics.'\n",
    "> * The twitter_archive table had four columns (doggo, floofer, pupper, puppo) representing a single variable dog stage\n",
    "> * All three datasets have similar observations that could be merged into one table with duplicate columns removed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e77b4bcb",
   "metadata": {},
   "source": [
    "## Cleaning\n",
    "Before cleaning I created a copy of each dataset to work with. Then each of these quality and tidiness issues were cleaned as well as a few other issues that were noticed when re-assessing the cleaned data. Once done iterating through each step in the cleaning process (define, code and test), the data was then saved as a single file twitter_archive_master.csv. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:py3_env] *",
   "language": "python",
   "name": "conda-env-py3_env-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
